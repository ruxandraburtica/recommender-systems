{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_memory_based_collaborative_filtering.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ruxandraburtica/recommender-systems/blob/master/1_memory_based_collaborative_filtering.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "avRbNXr3L3k4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Memory-based collaborative filtering\n"
      ]
    },
    {
      "metadata": {
        "id": "yOGMZHMzLzzb",
        "colab_type": "toc"
      },
      "cell_type": "markdown",
      "source": [
        ">[Memory-based collaborative filtering](#scrollTo=avRbNXr3L3k4)\n",
        "\n",
        ">[Memory-based collaborative filtering](#scrollTo=4amzxC539k88)\n",
        "\n",
        ">>>[When do you use collaborative filtering?](#scrollTo=4amzxC539k88)\n",
        "\n",
        ">>>[Types of collaborative filtering](#scrollTo=4amzxC539k88)\n",
        "\n",
        ">[Steps in building our collaborative filtering model:](#scrollTo=LmyTNkPNT4Uw)\n",
        "\n",
        ">>[Steps that we need to additionally run, in order to set up our environment:](#scrollTo=LmyTNkPNT4Uw)\n",
        "\n",
        ">>[Set up the environment](#scrollTo=9Zpoz98F81UV)\n",
        "\n",
        ">>>[0.1. Get additional packages](#scrollTo=YUMd8nTW-sjD)\n",
        "\n",
        ">>>[0.2. Import packages needed throughout the notebook](#scrollTo=Y3-_67QY9OoU)\n",
        "\n",
        ">>>[0.3. Get the data](#scrollTo=XHKSiVyO-wvq)\n",
        "\n",
        ">>[Metrics](#scrollTo=OxMMw3xxKndS)\n",
        "\n",
        ">>[Compute prediction of rating](#scrollTo=iJCVCvaoUoAP)\n",
        "\n",
        ">>>[2.1. Similarity measures](#scrollTo=iJCVCvaoUoAP)\n",
        "\n",
        ">>>[Similarities](#scrollTo=OPPw0bMcEa9j)\n",
        "\n",
        ">>>[2.2. Prediction:](#scrollTo=YtH57xKalh8W)\n",
        "\n",
        ">>>[Prediction](#scrollTo=ysJ7OhCIkrzm)\n",
        "\n",
        ">>>[Evaluate the model using cross-validation](#scrollTo=n-b2aFPo6kTK)\n",
        "\n",
        ">>[Return top k products, similar to a given one](#scrollTo=2p-tDl8NA-Um)\n",
        "\n",
        ">>>[Note for the trainers: I was thinking this part to be hands-on for participants](#scrollTo=2p-tDl8NA-Um)\n",
        "\n",
        ">[Challenges in building a user-based collaborative filtering](#scrollTo=H29EP6EUIZAs)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4amzxC539k88",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Memory-based collaborative filtering\n",
        "\n",
        "Recommend items based only on past user's behaviour.\n",
        "\n",
        "\n",
        "### When do you use collaborative filtering?\n",
        "- you don't have to have any domain expertise in terms of what you're recommending (e.g. books, movies, music, car)\n",
        "- leverage relationship between users\n",
        "\n",
        "### Types of collaborative filtering\n",
        "- user-based\n",
        "- item-based\n"
      ]
    },
    {
      "metadata": {
        "id": "LmyTNkPNT4Uw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Steps in building our collaborative filtering model:\n",
        "1. Identify the metrics you want to use for the target user\n",
        "2. For each product, compute a prediction of rating that the target user would give to the product\n",
        "    - Identify the set of users most similar to the target user according to a similarity function (e.g. neighbourhood function)\n",
        "    - Identify the products these similar users liked\n",
        "    - For each of the products identified at the previous step, compute a prediction of rating that the target user would give to the product\n",
        "3. Return top K products\n",
        "\n",
        "## Steps that we need to additionally run, in order to set up our environment:\n",
        "\n",
        "- Get additional packages (they can be installed in the colab environment)\n",
        "- Imports \n",
        "- Get the data\n",
        "- Analyze the data"
      ]
    },
    {
      "metadata": {
        "id": "9Zpoz98F81UV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0. Set up the environment"
      ]
    },
    {
      "metadata": {
        "id": "YUMd8nTW-sjD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 0.1. Get additional packages\n",
        " In Jupyter notebooks and colab, we can install additional packages"
      ]
    },
    {
      "metadata": {
        "id": "-IMXMBgI9GB6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall -y scipy\n",
        "!pip install scipy==1.0.0\n",
        "!pip install surprise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3-_67QY9OoU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note:** After installing, please restart the runtime (Runtime --> Restart runtime)\n",
        "\n",
        "\n",
        "### 0.2. Import packages needed throughout the notebook"
      ]
    },
    {
      "metadata": {
        "id": "YLHChfxa9eWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "\n",
        "from surprise import Dataset, Reader, KNNBasic\n",
        "from surprise.model_selection import cross_validate, split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XHKSiVyO-wvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 0.3. Get the data\n",
        "\n",
        "\n",
        "We're going to use the MovieLens 100K dataset:\n",
        "https://grouplens.org/datasets/movielens/100k/\n",
        "\n",
        "\n",
        "We're going to download data ourselves, given that with your projects, probably data will not be available within `surprise`.\n",
        "\n",
        "We are going to use the data in the `u.data` file that contains all the user-item ratings. \n",
        "\n",
        "In the u.data file each line represents a rating from a user to an item and the time when the rating happened. \n",
        "\n",
        "The format of each line is:\n",
        "`userID itemID rating timestamp`, separated by tabs\n",
        "\n",
        "In order to read the data, we're creating a Reader and define its format. In this case each line is divided as user item rating timestamp and is seperated by a tab \\t. After we define the format we load our data in a Dataset object:"
      ]
    },
    {
      "metadata": {
        "id": "pvVqGHn5-5MQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download archive and extract its contents.\n",
        "ml_100k_url = 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
        "r = requests.get(ml_100k_url, stream=True)\n",
        "z = zipfile.ZipFile(BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "# Define the format\n",
        "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
        "\n",
        "# Load the data from the file using the reader format\n",
        "data = Dataset.load_from_file('./ml-100k/u.data', reader=reader)\n",
        "\n",
        "# Show first entries\n",
        "print('{:>8} {:>8} {:>8} {:>8}'.format('user_id', 'item_id', 'rating', 'timestamp'))\n",
        "for index, item in enumerate(data.raw_ratings):\n",
        "  print('{:>8} {:>8} {:>8} {:>8}'.format(*item))\n",
        "  if index > 5:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OxMMw3xxKndS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Metrics\n",
        "\n",
        "We're going to use the movie ratings as the metric for our collaborative filtering model.\n",
        "\n",
        "\n",
        "\n",
        "Details about data contained in the zip archive can be found here:\n",
        "\n",
        "http://files.grouplens.org/datasets/movielens/ml-100k/README"
      ]
    },
    {
      "metadata": {
        "id": "iJCVCvaoUoAP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Compute prediction of rating\n",
        "\n",
        "\n",
        "In oder to compute the similarity between users, most used methods are:\n",
        "* **`Pearson Correlation`**  \n",
        "* **`cosine similarity`**\n",
        "\n",
        "\n",
        "We have the following as input data:\n",
        "* A collection of users, $u_i$, $i=1, ...n$\n",
        "* A collection of products $p_j$, $j=1, ...m$\n",
        "* An $n$ x $m$ matrix of ratings $v_{ij}$, with $v_{ij}=?$ if user $i$ did not rate product $j$\n",
        "\n",
        "\n",
        "### 2.1. Similarity measures\n",
        "\n",
        "**Inner product:**\n",
        "\n",
        "Basic similarity function:\n",
        "$$inner(u_i, u_j) = \\sum_{k=1}^{m} v_{ik} v_{jk}$$\n",
        "\n",
        "**Cosine similarity:**\n",
        "\n",
        "The inner product, however, is unbounded. \n",
        "\n",
        "One way to make it bounded between -1 and 1 is to divide by the vectorsâ€™ L2 norms, giving the cosine similarity:\n",
        "$$cos(u_i, u_j) = \\frac{\\sum_{k=1}^{m} v_{ik} v_{jk}}{\\sqrt{\\sum_{k=1}^{m} v_{ik}^2\\sum_{k=1}^{m} v_{jk}^2}}$$\n",
        "\n",
        "\n",
        "**Pearson correlation:**\n",
        "\n",
        "Cosine similarity is not invariant to shifts. What is invariant, though, is the Pearson correlation because it takes into account the mean of the vectors. Let $v_i$ and $v_k$ be the means of the two vectors:\n",
        " \n",
        "$$u_{ik} = \\frac{\\sum_{j} (v_{ij} - v_{i})(v_{kj} - v_{k})}{\\sqrt{\\sum_{j} (v_{ij} - v_i)^2\\sum_{j} (v_{kj} - v_k)^2}}$$\n",
        "\n",
        "\n",
        "**Computing similarities**\n",
        "\n",
        "|  | HP1 | HP2 | HP3 | TW | SW1 | SW2 | SW3 |\n",
        "|---|---|---|---|---|---|---|---|\n",
        "| A |4| | | 5 | 1 | | |\n",
        "| B |5|5|4| | | | |\n",
        "| C | | | |2|4|5| |\n",
        "| D | |3 | | | | |3 |\n",
        "| E | 5|2 | | |  | |  1|\n",
        "| F |  |4 | | | 4 | |  3|\n",
        "\n",
        "\n",
        "We can intuitively see that A and B have similar preferences, while A and C have different ones.\n",
        "However, computing similarity by handling not rated movies with 0 does not reflect this difference that good.  Hence, we're going for pearson correlation, and the first step is computing the mean rating for each user: \n",
        "\n",
        "|  | HP1 | HP2 | HP3 | TW | SW1 | SW2 | SW3 | Mean |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| A |4| | | 5 | 1 | | |10/3|\n",
        "| B |5|5|4| | | | |14/3|\n",
        "| C | | | |2|4|5| |11/3|\n",
        "| D | |3 | | | | |3 |3|\n",
        "| E | 5|2 | | |  | |  1|8/3|\n",
        "| F |  |4 | | | 4 | |  3|11/3|\n",
        "\n",
        "Some people tend to be tough raters, while others tend to be easy raters. By substracting the mean of each row, we normalize the ratings, centering data around 0. This is why this is also called the centered cosine similarity.\n",
        "\n",
        "|  | HP1 | HP2 | HP3 | TW | SW1 | SW2 | SW3 | Mean |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| A |2/3| | | 5/3 | -7/3 | | |10/3|\n",
        "| B |1/3|1/3|-2/3| | | | |14/3|\n",
        "| C | | | |-5/3|1/3|4/3| |11/3|\n",
        "| D | |0 | | | | |0 |3|\n",
        "| E | 7/3|-2/3 | | |  | |  -5/3|8/3|\n",
        "| F |  |1/3 | | | 1/3 | |  -2/3|11/3|\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OPPw0bMcEa9j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Similarities\n",
        "\n",
        "Hands-on exercise: Compute the pearson correlation between user A and user B above\n",
        "\n",
        "Answer: **0.09**\n",
        "\n",
        "--- \n",
        "\n",
        "Individual exercise: Compute the pearson correlation between user A and user C above\n",
        "\n",
        "Answer: **?**"
      ]
    },
    {
      "metadata": {
        "id": "YtH57xKalh8W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.2. Prediction:\n",
        "We compute the prediction for user $i$ and product $j$ as a weighted average:\n",
        "\n",
        "$$v_{ij}^\\star = \\frac{\\sum_{v_{kj} \\neq ?} u_{ik} v_{kj}}{\\sum_{v_{kj}} u_{ik}}$$\n",
        "\n",
        "\n",
        "\n",
        "Here, $u_{ik}$ is the similarity between user $i$ and user $k$ and $v_{k, j}$ is the rating that user $k$ gives to item $j$ with $v_{k, j} = ?$ if the user has not rated that item. \n",
        "\n",
        "\n",
        "**Compute predictions:**\n",
        "\n",
        "Starting off from the previous matrix and having computed the similarity scores:\n",
        "\n",
        "\n",
        "|  | HP1 | HP2 | HP3 | TW | SW1 | SW2 | SW3 | Similarity |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| A |4| | | 5 | 1 | | |1|\n",
        "| B |5|5|4| | | | |0.09|\n",
        "| C | | | |2|4|5| ||\n",
        "| D | |3 | | | | |3 ||\n",
        "| E | 5|2 | | |  | |  1||\n",
        "| F |  |4 | | | 4 | |  3||\n",
        "\n",
        "Let's say that we're using 2 closest neighbours to do predictions. In this case, for A, we would choose E and B, because they have the highest similarity scores.\n",
        "\n",
        "\n",
        "|  | HP1 | HP2 | HP3 | TW | SW1 | SW2 | SW3 | Similarity |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| A |4| 2.24| | 5 | 1 | | |1|\n",
        "| B |5|5|4| | | | |0.09|\n",
        "| C | | | |2|4|5| ||\n",
        "| D | |3 | | | | |3 ||\n",
        "| E | 5|2 | | |  | |  1||\n",
        "| F |  |4 | | | 4 | |  3||"
      ]
    },
    {
      "metadata": {
        "id": "ysJ7OhCIkrzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prediction\n",
        "\n",
        "Compute predictions for the other items of user A and complete the table below:\n",
        "\n",
        "\n",
        "|  | HP1 | HP2 | HP3 | TW | SW1 | SW2 | SW3 | Similarity |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| A |4| 2.24|  | 5 | 1 | |  |1|\n",
        "| B |5|5|4| | | | |0.09|\n",
        "| C | | | |2|4|5| ||\n",
        "| D | |3 | | | | |3 ||\n",
        "| E | 5|2 | | |  | |  1||\n",
        "| F |  |4 | | | 4 | |  3||\n"
      ]
    },
    {
      "metadata": {
        "id": "jX40Q_oh_UNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We're going to use `KNNBasic` collaborative filtering algorithm in order to build an user-based recommender system.\n",
        "# We're going to use `pearson` correlation.\n",
        "\n",
        "sim_options = {\n",
        "    'name': 'pearson_baseline',\n",
        "    'user_based': False\n",
        "}\n",
        "\n",
        "# Fit to trainset.\n",
        "algo = KNNBasic(sim_options=sim_options)\n",
        "trainset = data.build_full_trainset()\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Predict the rating for a specific (user, item) pair.\n",
        "# Note that we know the actual rating.\n",
        "uid = str(196)\n",
        "itemid = str(302)\n",
        "actual_rating = 4\n",
        "algo.predict(uid=uid, iid=itemid, r_ui=actual_rating, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n-b2aFPo6kTK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model using cross-validation\n",
        "\n",
        "Our model will try to optimize predictions, in order to match as closely as possible the actual results.\n",
        "\n",
        "As we're trying to predict the rating of a certain user-movie combination, we will compare that prediction to the actual rating. The difference between the actual and the predicted rating is measured using classical error measurements such as Root mean squared error (RMSE) and Mean absolute error (MAE):\n",
        "\n",
        "\n",
        "$$RMSE = \\sqrt{\\sum_{t=1}^{T}  {(\\hat{y_t}-y_t)}^2 \\over n}$$\n",
        "\n",
        "$$MAE = {\\sum_{t=1}^{T}  (\\hat{y_t}-y_t) \\over n}$$\n",
        "\n",
        "Here, $\\hat{y_t}$ is the prediction of $y_t$.\n",
        "\n",
        "We're going to use collaborative filtering to predict what rating would a `(user, movie)` parent have, given all the other ratints. We're going to do that using SVD."
      ]
    },
    {
      "metadata": {
        "id": "OcY2bdejSJpZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kf = split.KFold(random_state=0, n_splits=5)\n",
        "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=kf, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2p-tDl8NA-Um",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Return top k products, similar to a given one\n",
        "\n",
        "\n",
        "In oder to compute the similarity between users, most used methods are **`Pearson Correlation`** and **`cosine similarity`**. \n"
      ]
    },
    {
      "metadata": {
        "id": "VLfb5pcKA6vz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "from surprise import get_dataset_dir\n",
        "\n",
        "k = 5\n",
        "\n",
        "def read_item_names():\n",
        "    \"\"\"Read the u.item file from MovieLens 100-k dataset and return two\n",
        "    mappings to convert raw ids into movie names and movie names into raw ids.\n",
        "    \"\"\"\n",
        "\n",
        "    file_name = './ml-100k/u.item'\n",
        "    rid_to_name = {}\n",
        "    name_to_rid = {}\n",
        "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
        "        for line in f:\n",
        "            line = line.split('|')\n",
        "            rid_to_name[line[0]] = line[1]\n",
        "            name_to_rid[line[1]] = line[0]\n",
        "\n",
        "    return rid_to_name, name_to_rid\n",
        "  \n",
        "  \n",
        "# Read the mappings raw id <-> movie name\n",
        "rid_to_name, name_to_rid = read_item_names()\n",
        "\n",
        "# Retrieve inner id of the movie Toy Story\n",
        "toy_story_raw_id = name_to_rid['Toy Story (1995)']\n",
        "toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)\n",
        "\n",
        "# Retrieve neighbour ids of the nearest neighbors of Toy Story.\n",
        "# Note: You can use the get_neighbours method:\n",
        "# http://surprise.readthedocs.io/en/stable/algobase.html#surprise.prediction_algorithms.algo_base.AlgoBase.get_neighbors\n",
        "\n",
        "# We need the neoighbour names, so we're going to use this part to convert from IDs to movie names.\n",
        "toy_story_neighbors = (algo.trainset.to_raw_iid(inner_id)\n",
        "                       for inner_id in toy_story_neighbors)\n",
        "toy_story_neighbors = (rid_to_name[rid]\n",
        "                       for rid in toy_story_neighbors)\n",
        "\n",
        "print('The {} nearest neighbors of Toy Story are:'.format(k))\n",
        "for movie in toy_story_neighbors:\n",
        "    print(movie)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H29EP6EUIZAs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Challenges in building a user-based collaborative filtering\n",
        "\n",
        "* Sparsity of the (user, item) matrix \n",
        "    - large product sets, but users rank about 1% of the products\n",
        "    - millions of users, and difficult to find users that have rated same movies\n",
        "* Difficult to make predictions based on nearest-neighbours algorithms\n",
        "* Scalability - the complexity grows with the number of products and users\n",
        "\n",
        "**Next**: use latent models to capture similarity between users and items in a reduced dimensional space:\n",
        "* Matrix factorization\n",
        "* Clustering\n",
        "* Projection (PCA)\n"
      ]
    }
  ]
}